{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 8 - Parallelism in Julia\n",
    "\n",
    "Today we'll talk a bit about Julia's built-in tecniques for taking advantage of parallelism, as well as Julia's MPI interface and using GPUs.\n",
    "\n",
    "* See [farmshare.md](farmshare.md) to see how to connect to Stanford's farmshare cluster.\n",
    "* [Julia's Parallel Documentation](https://docs.julialang.org/en/stable/manual/parallel-computing)\n",
    "\n",
    "## Example: Monte Carlo Simulations\n",
    "\n",
    "One of the many ways that computers have aided science is through simulation.  Sometimes you may not have a closed-form way to access a quantity of interest, and can obtain a good guess through running many simulations with parameters drawn from a distribution, and looking at the average behavior of your model.  This class of methods is known as [Monte Carlo methods](https://en.wikipedia.org/wiki/Monte_Carlo_method).  \n",
    "\n",
    "One of the benefits of Monte Carlo methods is that they are often trivially parallelizable, since you can run independent experiments on separate processes, and then aggregate the results in a single round of communication at the end.\n",
    "\n",
    "One of the great uses of Monte Carlo methods is [integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration), which becomes increasingly attractive over high-dimensional domains.  The cannonical example is estimating $\\pi$ by integrating a circle on a square domian.\n",
    "\n",
    "The area of a circle with unit radius is $\\pi r^2 = \\pi$.\n",
    "The area of a square on $[-1, 1]^2$ is 4.  If we place the unit circle in this square, the ratio of their areas is $\\pi/4$.  The idea is that we sample uniformly on this square, and then see what portion of the points lie in the circle.  We know that this ratio should be approximately $\\pi/4$, so re multiply the ratio by 4 to obtain our approximation of $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "θ = linspace(0,2π,1000)\n",
    "circ_xs = cos.(θ)\n",
    "circ_ys = sin.(θ)\n",
    "\n",
    "sq_xs = [ 1, 1,-1,-1, 1]\n",
    "sq_ys = [-1, 1, 1,-1,-1]\n",
    "\n",
    "r_xs = rand(100) * 2 - 1\n",
    "r_ys = rand(100) * 2 - 1\n",
    "\n",
    "xkcd()\n",
    "plot(circ_xs,circ_ys, \"b-\", sq_xs, sq_ys, \"g-\", r_xs, r_ys, \"r.\")\n",
    "axis([-1.2, 1.2, -1.1, 1.1])\n",
    "title(\"Setup\")\n",
    "show()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function π_monte_carlo(n_samples::Int64)\n",
    "    n_circle = 0\n",
    "    for i=1:n_samples\n",
    "        x = rand() * 2 - 1\n",
    "        y = rand() * 2 - 1\n",
    "        r2 = x^2 + y^2\n",
    "        if r2 <= 1\n",
    "            n_circle += 1\n",
    "        end\n",
    "    end\n",
    "    return (n_circle / n_samples) * 4\n",
    "end\n",
    "\n",
    "@time π_monte_carlo(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = Array{Float64}(0)\n",
    "n_pts = 2.^(6:30) # 2^30 ≈ 1 billion\n",
    "for n_samples in n_pts\n",
    "    println(\"n_samples = $n_samples\")\n",
    "    tic()\n",
    "    push!(errors, π_monte_carlo(n_samples) - π)\n",
    "    t = toq()\n",
    "    tstr = @sprintf(\"%.3e\",t)\n",
    "    println(\"  $tstr sec.\")\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "xkcd(false)\n",
    "show()\n",
    "zx = [1, maximum(n_pts)]\n",
    "zy = [0, 0]\n",
    "semilogx(zx, zy ,\"k--\", n_pts, errors, \".r\" )\n",
    "title(\"Error of MC calculation\")\n",
    "show()\n",
    "\n",
    "exp_acc = 1.0 ./ sqrt.(n_pts)\n",
    "\n",
    "loglog(n_pts, exp_acc, \"b--\", n_pts, abs.(errors), \".r\")\n",
    "title(\"Error of MC calculation vs. CLT\")\n",
    "show()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes us ~10 sec. to estimate $\\pi$ on a billion points. However, this used only one core on the machine.  What if we want to use more?  \n",
    "\n",
    "## Using more than one process\n",
    "\n",
    "If you're starting up Julia in a terminal, you can use\n",
    "```\n",
    "julia -j 4\n",
    "```\n",
    "To indicate that 4 processes are available.  If you're using a IJulia notebook or already have julia running, you can add processes with `addprocs()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show nprocs()\n",
    "addprocs(3)\n",
    "@show nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(my machine has 4 cores).  Now, Julia will know that it can use up to 4 separate processes, but when you're writing code, you you need to explicitly use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spawns process that generates a random matrix\n",
    "m = @spawn randn(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the answer from the process we spawned\n",
    "fetch(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also request that the call spawn on a particular process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spawn at process 2\n",
    "m = @spawnat 2 randn(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same thing can be done with the remotecall function\n",
    "m = remotecall(randn, 4, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to make sure that all processes have access to functions that you define.  In order to define a function on all processes use the `@everywhere` macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_test(a, b)\n",
    "    a + b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = @spawn sum_test(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function sum_test(a, b)\n",
    "    a + b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = @spawn sum_test(1,2)\n",
    "fetch(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're loading modules, `using` will load them on all processes, but `include()` will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using Distributions\n",
    "# spawn a process to generate an exponential random variable\n",
    "d = @spawn Exponential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = fetch(d)\n",
    "rand(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control which process is used with `@spawnat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = @spawnat 2 1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a result immediately, you can use `remotecall_fetch()`. The first argument is the function to call.  The second argument is the process number to spawn the call at.  The remaining arguments are the inputs to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remotecall_fetch(+, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spawn/fetch commands are great for spinning off function evaluations.  If you want something that looks like a parallel for-loop, you can use the `@parallel` macro.  Note that this example does something like a MPI gather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nheads = @parallel (+) for i=1:200000000\n",
    "  Int(rand(Bool))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use a parallel for-loop, you should make sure that the inner contents of the loop are independent of each other, since you aren't controlling evaluation order, or which process is doing what.  Note that every process also uses its own copy of data, so something like the following will not work like you might wish it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rand(100)\n",
    "@parallel for i = 1:100\n",
    "    a[i] = 1\n",
    "end\n",
    "@show a\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "* Modify the Monte Carlo calculation of $\\pi$ above to use more than one process. (There's more than one way to do this!)\n",
    "* How fast is your modified version compared to the single process version?\n",
    "* Can you add the `@simd` macro to the for-loop?  How does this compare with parallelization?  Can you mix parallelization and `@simd`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays\n",
    "\n",
    "As mentioned above, if you wish to have multiple processes work on a single array you need to go above an beyond the limitiations of a standard array.  There are two types of arrays you may wish to use for parallel/distributed computations:\n",
    "\n",
    "* [Shared Arrays](https://docs.julialang.org/en/stable/manual/parallel-computing.html#man-shared-arrays-1) - built into Julia - all processes can access any element of the array\n",
    "* [Distributed Arrays](https://github.com/JuliaParallel/DistributedArrays.jl) - package through JuliaParallel - array elements are distributed over processes.\n",
    "\n",
    "### Shared Arrays\n",
    "\n",
    "[Shared Arrays](https://docs.julialang.org/en/stable/manual/parallel-computing.html#man-shared-arrays-1) are accessible from all processes and maintain the same data.  This is useful for taking advantage of all processes on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_add = 4 - nprocs()\n",
    "addprocs(n_add)\n",
    "@show nprocs()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = SharedArray{Int64}((3,4), init = S -> S[Base.localindexes(S)] = myid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument is the type of the array elements, the second argument is a tuple of dimensions, the (optional) third argument is an initialization function, and the (optional) fourth argument denotes what processes the Shared Array should be shared on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = SharedArray{Int64}((3,4), init = S -> S[indexpids(S):length(procs(S)):length(S)] = myid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = @spawnat 2 S[1] = 7\n",
    "fetch(p)\n",
    "S # displayed by process 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test matrix-vector mulitplication\n",
    "n = 1000\n",
    "A = randn(n,n)\n",
    "S = SharedArray{Float64}(A)\n",
    "x = randn(n)\n",
    "@time bA = A*x\n",
    "@time bS = S*x\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the @parallel macro with SharedArrays\n",
    "n = 10\n",
    "v = SharedArray{Int64}(n)\n",
    "@parallel for i = 1:n\n",
    "   v[i] = i \n",
    "end\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Arrays\n",
    "\n",
    "[Distributed Arrays](https://github.com/JuliaParallel/DistributedArrays.jl) are offered from [Julia Parallel](https://github.com/JuliaParallel) to distribute an array over several processes.  If you've done distributed linear algebra before, this is probably more familiar than the SharedArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DistributedArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = drandn(5, 5)\n",
    "println(B)\n",
    "A = @DArray [i+j for i = 1:5, j = 1:5]\n",
    "println(A)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution pattern of array\n",
    "A = @DArray [myid() for i=1:10, j=1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = @DArray [myid() for i = 1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed arrays will change the order of operations, so you should expect to see differences due to floating point errors when you perform operations with DArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = randn(100,100)\n",
    "@show s1 = sum(A)\n",
    "D = distribute(A)\n",
    "@show s2 = sum(D)\n",
    "@show s1 - s2\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test matrix-vector mulitplication\n",
    "n = 2000\n",
    "A = randn(n,n)\n",
    "@show size(A)\n",
    "D = distribute(A)\n",
    "x = randn(n)\n",
    "dx = distribute(x)\n",
    "@show size(x)\n",
    "@time bA = A*x\n",
    "@time bD = D*dx\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(bD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "* Can you use linear algebra routines (such as SVD) on SharedArrays?  What about DArrays?  What is the type of the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Julia with MPI\n",
    "\n",
    "Most of what we've considered above is useful for using multiple cores on a single machine.  What if you want to run Julia on a cluster?  Julia provides a [MPI package](https://github.com/JuliaParallel/MPI.jl) that allows you to run Julia scripts with `mpirun` just as you would with C/Fortran binaries.\n",
    "\n",
    "If you haven't used it before, [MPI](https://computing.llnl.gov/tutorials/mpi/) is the Message Passing Interface Standard.  The standard has several implementations - the most popular are [MPICH](https://www.mpich.org/) and [OpenMPI](http://www.open-mpi.org/) (both are open source), which you can get for your system using almost any repository manager.  Corn.stanford has OpenMPI, I have MPICH. In practice, the two are mostly interchangable, although they use slightly different syntax if you use a [hostfile](https://www.open-mpi.org/faq/?category=running).\n",
    "\n",
    "If you haven't used MPI before, it is worth looking at it a bit if you are interested in scientific computing.  The appeal in Julia is that it allows you finer control when writing algorithms that may use several cores or machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example\n",
    "See the script [here](mpi_test.jl)\n",
    "```bash\n",
    "mpirun -np 4 julia mpi_test.jl\n",
    "```\n",
    "(note that if julia is an alias, then mpirun will throw an error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use Julia's MPI interface to simply initialize MPI, and then call C/Fortran libraries that use it more extensively.  This is what we see with the Elemental package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPUs with CuArrays\n",
    "\n",
    "See [cuarrays.jl](cuarrays.jl) for an example script.  Also check out [Flux.jl](https://fluxml.github.io/Flux.jl/stable/) if you want to see a deep learning package entirely in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "* try running one of the [examples](https://github.com/JuliaParallel/MPI.jl/tree/master/examples) in MPI.jl on corn.stanford."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extras\n",
    "\n",
    "There's a lot more out there to explore with Julia and parallelism.\n",
    "\n",
    "* You can use Julia as a [cluster manager](https://docs.julialang.org/en/stable/manual/parallel-computing.html#ClusterManagers-1)\n",
    "* There's exprimental support for [threading](https://docs.julialang.org/en/stable/manual/parallel-computing.html#Multi-Threading-(Experimental)-1)\n",
    "* Check out [JuliaParallel](https://github.com/JuliaParallel) to see a variety of packages for parallel and out-of-core computation\n",
    "    * Wrappers for distributed linear algebra/ODEs - [ScaLAPACK.jl](https://github.com/JuliaParallel/ScaLAPACK.jl), [Elemental.jl](https://github.com/JuliaParallel/Elemental.jl), [PETSc.jl](https://github.com/JuliaParallel/PETSc.jl)\n",
    "    * Data storage - [HDFS.jl](https://github.com/JuliaParallel/HDFS.jl)\n",
    "    * Parallel/out-of-core - [Dagger.jl](https://github.com/JuliaParallel/Dagger.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
