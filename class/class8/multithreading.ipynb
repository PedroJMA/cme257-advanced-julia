{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 8 - Multithreading in Julia\n",
    "\n",
    "Today we'll talk a bit about Julia's built-in tecniques for taking advantage of multithreading. Note that this notebook makes use of some new features in Julia 1.3.0. If you would like to follow along please install the current release candidate (rc5).\n",
    "\n",
    "* [Julia's Parallel Documentation](https://docs.julialang.org/en/v1/manual/parallel-computing)\n",
    "\n",
    "## Getting Started\n",
    "To get started, launch Julia 1.3.0-rc5 with the command `JULIA_NUM_THREADS=4 [wherever your Julia binary is]` before opening this notebook. On a Mac, after moving the new Julia to the Applications folder the command is `JULIA_NUM_THREADS=4 /Applications/Julia-1.3.app/Contents/Resources/julia/bin/julia`\n",
    "\n",
    "\n",
    "You can test if this worked by running `Threads.nthreads()`-- the output should be 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Before we get started with multithreading, we'll briefly go over Julia's interface for Tasks. A Task is essentially a fully specified function call that we have put aside for later. We can create a `Task` with `@task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f() = println(\"Hello\")\n",
    "function g(x)\n",
    "    sleep(2)\n",
    "    println(x)\n",
    "    return x\n",
    "end\n",
    "v = 1\n",
    "#Examples of tasks\n",
    "task1 = @task f()\n",
    "task2 = @task (global v = g(\"Hello again\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if a task has been started with `istaskstarted()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show istaskstarted(task1)\n",
    "@show istaskstarted(task2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start a task with `schedule()` and check if it is finished with `istaskdone()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule(task2);\n",
    "schedule(task1);\n",
    "sleep(0.1)\n",
    "println(\"Checking if both tasks are started:\")\n",
    "@show istaskstarted(task1)\n",
    "@show istaskstarted(task2);\n",
    "sleep(1)\n",
    "println(\"Which tasks are done after one second?\")\n",
    "@show istaskdone(task1)\n",
    "@show istaskdone(task2)\n",
    "sleep(2)\n",
    "println(\"Is task2 done after 2 more seconds?\")\n",
    "@show istaskdone(task2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to capture the output of a function that we run as a Task, we can alternatively use `fetch()`. Keep in mind that you need to schedule the task before you can fetch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3 = @task(g(1))\n",
    "schedule(task3)\n",
    "v = fetch(task3)\n",
    "@show v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax `@async` conveniently wraps an expression in a Task and schedules it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = zeros(3)\n",
    "function h!(v,i)\n",
    "    sleep(4-i)\n",
    "    println(i)\n",
    "    v[i] = i\n",
    "end\n",
    "for i = 1:3\n",
    "    @async h!(vec,i)\n",
    "end\n",
    "@show vec;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that in the above the vector `vec` is showed before its entries are updated: the `@show` and the calls to `h!` are run asynchronously. If we would like to wait for all asynchronous tasks to complete running before continuing down the code, we can wrap the for loop with the `@sync` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = zeros(3)\n",
    "@sync for i = 1:3\n",
    "    @async h!(vec,i)\n",
    "end\n",
    "@show vec;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that Tasks by themselves all run in a single thread (even when called with `@async`). Thus in compute-bound workloads we do not obtain a performance improvement over not using `Tasks`. However, for network and I/O bound programs scheduling jobs to run asynchronously can give a significant improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "This exercise should work in Julia 1.2 .\n",
    "\n",
    "* Implement [sleep sort](https://www.geeksforgeeks.org/sleep-sort-king-laziness-sorting-sleeping/) using Tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading\n",
    "\n",
    "### Example: Monte Carlo Simulations\n",
    "\n",
    "One of the many ways that computers have aided science is through simulation.  Sometimes you may not have a closed-form way to access a quantity of interest, and can obtain a good guess through running many simulations with parameters drawn from a distribution, and looking at the average behavior of your model.  This class of methods is known as [Monte Carlo methods](https://en.wikipedia.org/wiki/Monte_Carlo_method).  \n",
    "\n",
    "One of the benefits of Monte Carlo methods is that they are often trivially parallelizable, since you can run independent experiments on separate processes, and then aggregate the results in a single round of communication at the end.\n",
    "\n",
    "One of the great uses of Monte Carlo methods is [integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration), which becomes increasingly attractive over high-dimensional domains.  The cannonical example is estimating $\\pi$ by integrating a circle on a square domian.\n",
    "\n",
    "The area of a circle with unit radius is $\\pi r^2 = \\pi$.\n",
    "The area of a square on $[-1, 1]^2$ is 4.  If we place the unit circle in this square, the ratio of their areas is $\\pi/4$.  The idea is that we sample uniformly on this square, and then see what portion of the points lie in the circle.  We know that this ratio should be approximately $\\pi/4$, so re multiply the ratio by 4 to obtain our approximation of $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function π_monte_carlo(n_samples::Int64)\n",
    "    n_circle = 0\n",
    "    for i=1:n_samples\n",
    "        x = rand() * 2 - 1\n",
    "        y = rand() * 2 - 1\n",
    "        r2 = x^2 + y^2\n",
    "        if r2 <= 1\n",
    "            n_circle += 1\n",
    "        end\n",
    "    end\n",
    "    return (n_circle / n_samples) * 4\n",
    "end\n",
    "\n",
    "errors = zeros(0)\n",
    "n_pts = 2 .^collect(6:30) # 2^30 ≈ 1 billion\n",
    "for n_samples in n_pts\n",
    "    t1 = time()\n",
    "    val = π_monte_carlo(n_samples) - π\n",
    "    push!(errors, val)\n",
    "    t2 = time()\n",
    "    println(\"Sampling $n_samples points took $(t2-t1) seconds and achieved error $val\")\n",
    "end\n",
    "using Plots\n",
    "plot(collect(6:30),abs.(errors),yscale=:log10,label=\"\",xlabel=\"Log of n_pts\",ylabel=\"Absolute Value Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes us ~25 sec. to estimate $\\pi$ on a billion points. However, this used only one core on the machine.  What if we want to use more?  \n",
    "\n",
    "## Using more than one thread\n",
    "\n",
    "From the way we started Julia, we know that it has access to four threads right now. However, we need to explicitly use them in our code. The `@spawn` macro creates a Task which Julia will automatically assign to a thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x000000010b036ad0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base.Threads.@spawn\n",
    "\n",
    "# spawns process that generates a random matrix\n",
    "m = @spawn randn(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the output from the task\n",
    "fetch(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that Julia will only give us the value of m if we `fetch` it. In the above example our task completed almost immediately. Of course we can also run more intensive computations than this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sleep1(x::Int64)\n",
    "    sleep(x)\n",
    "    #pretend that sleeping is hard work\n",
    "    println(\"Sleep printed $x\")\n",
    "    return 1\n",
    "end\n",
    "\n",
    "function awake(x::Int64)\n",
    "    println(\"Awake printed $x\")\n",
    "    return 2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = @spawn sleep1(5)\n",
    "out2 = @spawn awake(5)\n",
    "fetch(out) + fetch(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when two threads do not interact, they return their outputs asynchronously just as we expect. Thus `sleep1` prints second even though it was called first. However unlike with normal `Tasks` using `@spawn` genuinely allocates jobs to different threads: it can give speedups for compute-boundworkloads\n",
    "\n",
    "Remember that multithreading is only safe if we can ensure that the threads avoid touching the same memory. Thus when controlling the flow of multithreaded code it is sometimes useful to wait for a slow thread to catch up. We can do this by using the `wait` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = @spawn sleep1(5)\n",
    "out2 = @spawn sleep1(3)\n",
    "wait(out)\n",
    "out2 = @spawn awake(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this type of threading can give real speedups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "a = collect(1:10^8)\n",
    "function add1!(a,first,last)\n",
    "    for i = first:last\n",
    "        a[i] = 1+a[i]\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@benchmark add1!(a,1,10^8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = collect(1:10^8)\n",
    "@benchmark begin\n",
    "t1 = @spawn add1!(a,1,5*10^7)\n",
    "t2 = @spawn add1!(a,5*10^7+1,10^8)\n",
    "wait(t1); wait(t2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common use case for multithreading is for [embarassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) tasks. As an example, we might want to apply an operation to every element of a vector. We can conveniently do this with `Threads.@threads for`. This macro automatically divides up the range and assigns each chunk to one of the tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = collect(1:10)\n",
    "Threads.@threads for i = 1:10\n",
    "   println(\"i = $i on thread $(Threads.threadid())\")\n",
    "    a[i] = Threads.threadid()\n",
    "end\n",
    "@show a;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this knowledge in hand, we can write a multithreaded version of our Monte-Carlo calculator for π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function π_multithreaded(n_samples::Int64)\n",
    "    new_n_samples = div(n_samples,nthreads())\n",
    "    jobs = []\n",
    "    out = zeros(nthreads())\n",
    "    for i = 1:nthreads()\n",
    "        push!(jobs,@sync π_monte_carlo(new_n_samples))\n",
    "    end\n",
    "    for i = 1:nthreads()\n",
    "        out[i] = fetch(jobs[i])\n",
    "    end\n",
    "    return sum(out)/nthreads()\n",
    "end\n",
    "@time π_multithreaded1(2^30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 2.5x speedup! However, the following (plausible looking) code returns the wrong answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function π_multithreaded_bad(n_samples::Int64)\n",
    "    n_circle = 0\n",
    "    Threads.@threads for i=1:n_samples\n",
    "        x = rand() * 2 - 1\n",
    "        y = rand() * 2 - 1\n",
    "        r2 = x^2 + y^2\n",
    "        if r2 <= 1\n",
    "            n_circle += 1\n",
    "        end\n",
    "    end\n",
    "    return (n_circle / n_samples) * 4\n",
    "end\n",
    "@time π_multithreaded_bad(2^20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because each thread is trying to update `n_circle`-- the race conditions mean we get the wrong answer. We can fix this by using `Thread.Atomic`s. An atomic is a primitive (numerical) value which Julia will keep track of during multithreaded iteration. If we only do commutative operations to the global `n_circle`, the Atomic will group the operations within a thread and aggregate them at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function π_multithreaded_fixed(n_samples::Int64)\n",
    "    n_circle = Threads.Atomic{Int}(0)\n",
    "    Threads.@threads for i=1:n_samples\n",
    "        x = rand() * 2 - 1\n",
    "        y = rand() * 2 - 1\n",
    "        r2 = x^2 + y^2\n",
    "        if r2 <= 1\n",
    "            Threads.atomic_add!(n_circle,1)\n",
    "        end\n",
    "    end\n",
    "    return (n_circle.value / n_samples) * 4\n",
    "end\n",
    "@time π_multithreaded_fixed(2^30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is not much faster than the serial one. Although atomics in Julia enable us to use multiple threads, they are unfortunately very slow. We can speed this up by  manually partitioning the interval and performing the aggregation ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should work in Julia 1.2.0\n",
    "function π_multithreaded_better(n_samples::Int64)\n",
    "    results = zeros(Threads.nthreads())\n",
    "    Threads.@threads for tid = 1:Threads.nthreads()\n",
    "        n_circle = 0\n",
    "        len = div(n_samples, Threads.nthreads())\n",
    "        domain = ((tid-1)*len+1):(tid*len)\n",
    "        for i in domain\n",
    "            x = rand() * 2 - 1\n",
    "            y = rand() * 2 - 1\n",
    "            r2 = x^2 + y^2\n",
    "            if r2 <= 1\n",
    "                n_circle += 1\n",
    "            end\n",
    "        end\n",
    "        results[tid] = n_circle\n",
    "    end\n",
    "    return (sum(results) / n_samples) * 4\n",
    "end\n",
    "@time π_multithreaded_better(2^30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "* Can you add the `@simd` macro to the for-loop?  How does this compare with parallelization?  Can you mix parallelization and `@simd`?\n",
    "* Does `@inbounds` have an effect too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Advanced Multithreaded Algorithms\n",
    "\n",
    "The `Threads.@spawn` interface is extremely versatile and powerful. We can spawn millions of tasks, and so long as we don't care about the order that most computations run in we can reap the benefits of multiple cores while Julia takes care of the rest. Here is an example of a more complicated multithreaded algorithm, taken from [the announcement of the feature](https://julialang.org/blog/2019/07/multithreading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mergesort!(v, lo::Int=1, hi::Int=length(v))\n",
    "    if lo >= hi\n",
    "        return v\n",
    "    end\n",
    "    if hi - lo < 100000\n",
    "        sort!(view(v, lo:hi), alg = MergeSort)\n",
    "        return v\n",
    "    end\n",
    "    mid = (lo+hi)>>>1 \n",
    "    mergesort!(v, lo, mid) \n",
    "    mergesort!(v, mid+1, hi)              \n",
    "    temp = v[lo:mid]            \n",
    "    i, k, j = 1, lo, mid+1            \n",
    "    @inbounds while k < j <= hi\n",
    "        if v[j] < temp[i]\n",
    "            v[k] = v[j]\n",
    "            j += 1\n",
    "        else\n",
    "            v[k] = temp[i]\n",
    "            i += 1\n",
    "        end\n",
    "        k += 1\n",
    "    end\n",
    "    @inbounds while k < j\n",
    "        v[k] = temp[i]\n",
    "        k += 1\n",
    "        i += 1\n",
    "    end\n",
    "\n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "psort! (generic function with 4 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function psort!(v, lo::Int=1, hi::Int=length(v))\n",
    "    if lo >= hi                       # 1 or 0 elements; nothing to do\n",
    "        return v\n",
    "    end\n",
    "    if hi - lo < 100000               # below some cutoff, run in serial\n",
    "        sort!(view(v, lo:hi), alg = MergeSort)\n",
    "        return v\n",
    "    end\n",
    "    mid = (lo+hi)>>>1                 # find the midpoint\n",
    "\n",
    "    half = @spawn psort!(v, lo, mid)  # task to sort the lower half; will run\n",
    "    psort!(v, mid+1, hi)              # in parallel with the current call sorting\n",
    "                                      # the upper half\n",
    "    wait(half)                        # wait for the lower half to finish\n",
    "\n",
    "    temp = v[lo:mid]                  # workspace for merging\n",
    "\n",
    "    i, k, j = 1, lo, mid+1            # merge the two sorted sub-arrays\n",
    "    @inbounds while k < j <= hi\n",
    "        if v[j] < temp[i]\n",
    "            v[k] = v[j]\n",
    "            j += 1\n",
    "        else\n",
    "            v[k] = temp[i]\n",
    "            i += 1\n",
    "        end\n",
    "        k += 1\n",
    "    end\n",
    "    @inbounds while k < j\n",
    "        v[k] = temp[i]\n",
    "        k += 1\n",
    "        i += 1\n",
    "    end\n",
    "\n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: mergesort! not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: mergesort! not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at util.jl:155",
      " [2] top-level scope at In[14]:2"
     ]
    }
   ],
   "source": [
    "x = rand(10^7)\n",
    "@time mergesort!(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.230293 seconds (104.72 k allocations: 5.214 MiB)\n"
     ]
    }
   ],
   "source": [
    "x = rand(10^7)\n",
    "@time sort!(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.885546 seconds (62.70 k allocations: 308.295 MiB, 10.04% gc time)\n"
     ]
    }
   ],
   "source": [
    "x = rand(10^7)\n",
    "@time psort!(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the massive memory overhead (due to needing to pass around references to `Tasks`), our parallel sorting implementation outperformed the standard library one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "* Below is an implementation of the min-plus product, and a naive parallelization `par_minplus`. Is `par_minplus` faster?\n",
    "* Modify `par_minplus` to accomodate any number of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function minplus(a)\n",
    "    B = zeros(size(a))\n",
    "    n = size(a)[1]\n",
    "    for i = 1:n\n",
    "        for j = 1:n\n",
    "            value = n+1\n",
    "            for k = 1:n\n",
    "                value = min(value,a[i,k] + a[k,j])\n",
    "            end\n",
    "            B[i,j] = value\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return B\n",
    "end\n",
    "\n",
    "function minplus!(a,i_low = 1, i_hi = size(a)[1], B = zeros(size(a)))\n",
    "    n = size(a)[1]\n",
    "    for i = i_low:i_hi\n",
    "        for j = 1:n\n",
    "            value = n+1\n",
    "            for k = 1:n\n",
    "                value = min(value,a[i,k] + a[k,j])\n",
    "            end\n",
    "            B[i,j] = value\n",
    "        end\n",
    "    end\n",
    "    return B\n",
    "end\n",
    "\n",
    "function par_minplus(a,B=zeros(size(a)))\n",
    "    quarter_n = div(size(a)[1],4)\n",
    "    t1 = @spawn minplus!(a,1,quarter_n,B)\n",
    "    t2 = @spawn minplus!(a,quarter_n+1,2*quarter_n,B)\n",
    "    t3 = @spawn minplus!(a,2*quarter_n+1,3*quarter_n,B)\n",
    "    t4 = @spawn minplus!(a,3*quarter_n+1,4*quarter_n,B)\n",
    "    wait(t1); wait(t2); wait(t3); wait(t4)\n",
    "    return B\n",
    "end\n",
    "\n",
    "A = randn(400,400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locks\n",
    "\n",
    "As mentioned before, we need to be careful to avoid race conditions when writing multithreaded code. Each thread operates on the same space of memory by default, so if we aren't careful one thread might change something out from under another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function slow_sum(x)\n",
    "    sleep(1)\n",
    "    return sum(x)\n",
    "end\n",
    "x = ones(10)\n",
    "@spawn @show slow_sum(x) #should be 10\n",
    "@spawn x[1] = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this, we can use a `SpinLock()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ones(10)\n",
    "alock = ReentrantLock()\n",
    "@spawn begin\n",
    "lock(alock)\n",
    "println(\"Thread $(Threads.threadid()) obtained lock\")\n",
    "@show slow_sum(x,alock) #should be 10\n",
    "println(\"Thread $(Threads.threadid()) released lock\")\n",
    "unlock(alock)\n",
    "end\n",
    "@spawn begin\n",
    "lock(alock)\n",
    "println(\"Thread $(Threads.threadid()) obtained lock\")\n",
    "sleep(0.1)\n",
    "x[1] = 2\n",
    "println(\"Thread $(Threads.threadid()) released lock\")\n",
    "unlock(alock)\n",
    "end\n",
    "sleep(2)\n",
    "@show x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of locks in Julia that are worth mentioning: `SpinLock`s and `ReentrantLock`s. The main difference between the two is that a `SpinLock` can deadlock itself, while a `ReentrantLock` cannot. In code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alock = ReentrantLock()\n",
    "@spawn begin\n",
    "lock(alock)\n",
    "println(\"Thread $(Threads.threadid()) obtained lock\")\n",
    "lock(alock)\n",
    "println(\"Thread $(Threads.threadid()) obtained lock\")\n",
    "println(\"Thread $(Threads.threadid()) released lock\")\n",
    "unlock(alock)\n",
    "println(\"Thread $(Threads.threadid()) released lock\")\n",
    "unlock(alock)\n",
    "end;\n",
    "#Thread obtained a lock and tried to obtain it again without unlocking  it first. Julia recognized this and \n",
    "#prevented the thread from blocking itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alock = Threads.SpinLock()\n",
    "@spawn begin\n",
    "lock(alock)\n",
    "println(\"Thread $(Threads.threadid()) obtained lock\")\n",
    "#lock(alock)\n",
    "#println(\"Thread $(Threads.threadid()) obtained lock\")\n",
    "#println(\"Thread $(Threads.threadid()) released lock\")\n",
    "#unlock(alock)\n",
    "println(\"Thread $(Threads.threadid()) released lock\")\n",
    "unlock(alock)\n",
    "end;\n",
    "#When uncommented, this will kill the thread!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if a thread is holding a lock with `islocked()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alock = ReentrantLock()\n",
    "@spawn begin\n",
    "    lock(alock)\n",
    "    sleep(5)\n",
    "    unlock(alock)\n",
    "end\n",
    "@spawn begin\n",
    "    sleep(3)\n",
    "    @show islocked(alock)\n",
    "    sleep(3)\n",
    "    @show islocked(alock)\n",
    "end; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `trylock()` attempts to take a lock if it is available. Unlike `lock()`, calling `trylock()` means that the thread will not take the lock when it is available: it only cares if the lock is available in the moment. `trylock()` returns `true` if it succeeds in taking the lock and `false` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alock = ReentrantLock()\n",
    "@spawn begin\n",
    "    lock(alock)\n",
    "    sleep(5)\n",
    "    unlock(alock)\n",
    "end\n",
    "@spawn begin\n",
    "    sleep(3)\n",
    "    @show a = trylock(alock)\n",
    "    if a\n",
    "        unlock(alock) #trylock must be followed by unlock, if it succeeds\n",
    "        println(\"Took the lock\")\n",
    "    else\n",
    "        println(\"Failed to take lock\")\n",
    "    end\n",
    "    sleep(3)\n",
    "    @show a = trylock(alock)\n",
    "    if a\n",
    "        unlock(alock) #trylock must be followed by unlock, if it succeeds\n",
    "        println(\"Took the lock\")\n",
    "    else\n",
    "        println(\"Failed to take lock\")\n",
    "    end\n",
    "end; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia also supports very intricate message-passing schemes amongst `Tasks`, such as semaphore signalling and notifications. We only covered the basics here: if you would like to learn more about this check out the [documentation](https://docs.julialang.org/en/v1.4-dev/base/parallel/#Scheduling-1) or come talk to me after class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A couple small optimization tricks for Multithreaded Julia\n",
    "\n",
    "In Julia, there are two primary ways of obtaining thread safety: locks (which we just covered), and **thread-local state**. Locks are useful to ensure thread safety when the threads don't need to interact very often. However, oftentimes we might want to write code that shares memory amongst threads. We can see this in action in the implementation of `psort!` above:\n",
    "\n",
    "```julia\n",
    "function psort!(v, lo::Int=1, hi::Int=length(v))\n",
    "    if lo >= hi                  \n",
    "        return v\n",
    "    end\n",
    "    if hi - lo < 100000               # below some cutoff, run in serial\n",
    "        sort!(view(v, lo:hi), alg = MergeSort)\n",
    "        return v\n",
    "    end\n",
    "    \n",
    "    mid = (lo+hi)>>>1                 # These are the critical lines for us\n",
    "    half = @spawn psort!(v, lo, mid)  # \n",
    "    psort!(v, mid+1, hi)              # \n",
    "    wait(half)                        # \n",
    "    temp = v[lo:mid]                  # \n",
    "\n",
    "    i, k, j = 1, lo, mid+1            \n",
    "    @inbounds while k < j <= hi\n",
    "        if v[j] < temp[i]\n",
    "            v[k] = v[j]\n",
    "            j += 1\n",
    "        else\n",
    "            v[k] = temp[i]\n",
    "            i += 1\n",
    "        end\n",
    "        k += 1\n",
    "    end\n",
    "    @inbounds while k < j\n",
    "        v[k] = temp[i]\n",
    "        k += 1\n",
    "        i += 1\n",
    "    end\n",
    "\n",
    "    return v\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "There are two places where we would want to use shared memory in this implementation of Merge Sort. First, we obviously would like our threads to sort the array `v` in place and avoid unnecessary memory allocations. We accomplished thread safety by ensuring that no two `Tasks` ever saw overlapping ranges of the input: this is safe in Julia.\n",
    "\n",
    "Second and less obviously, we would like to reuse the `temp` array during the recursion. Right now, in every level of the recursion a new `temp` array is defined: eventually a thread will have many spurious `temp` arrays that it no longer needs from lower levels of the recursion. However, this is not obvious to do, as these arrays come from different tasks entirely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get around this issue, we utilize the fact that a `Task` can know which thread it is running in. Thus, the Task can write to a *thread's* `temp` array instead of creating a new one from scratch. This reusing of space is known as thread-local state: local variables during a recursion are reused within a thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modifications we need are:\n",
    "\n",
    "* Change the function signature to \n",
    "```julia\n",
    "function psort!(v, lo::Int=1, hi::Int=length(v), temps=[similar(v, 0) for i = 1:Threads.nthreads()])\n",
    "```\n",
    "* Change the recursive calls to `psort!` to use the temporary ranges;\n",
    "``` julia\n",
    "half = @spawn psort!(v, lo, mid, temps)\n",
    "psort!(v, mid+1, hi, temps)\n",
    "```\n",
    "* Modify the allocation of the `temp` array to stay within the ranges we defined on each thread\n",
    "``` julia\n",
    "temp = temps[Threads.threadid()]\n",
    "length(temp) < mid-lo+1 && resize!(temp, mid-lo+1)\n",
    "copyto!(temp, 1, v, lo, mid-lo+1)\n",
    "```\n",
    "\n",
    "Plugging these in, we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "psort2! (generic function with 4 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function psort2!(v, lo::Int=1, hi::Int=length(v), temps=[similar(v, 0) for i = 1:Threads.nthreads()])\n",
    "    if lo >= hi                  \n",
    "        return v\n",
    "    end\n",
    "    if hi - lo < 100000               # below some cutoff, run in serial\n",
    "        sort!(view(v, lo:hi), alg = MergeSort)\n",
    "        return v\n",
    "    end\n",
    "    \n",
    "    mid = (lo+hi)>>>1                 \n",
    "    half = @spawn psort2!(v, lo, mid, temps)\n",
    "    psort2!(v, mid+1, hi, temps)\n",
    "    wait(half)                         \n",
    "    temp = temps[Threads.threadid()]\n",
    "    length(temp) < mid-lo+1 && resize!(temp, mid-lo+1) #fancy way to implement an if-else statement\n",
    "    copyto!(temp, 1, v, lo, mid-lo+1)                \n",
    "\n",
    "    i, k, j = 1, lo, mid+1            \n",
    "    @inbounds while k < j <= hi\n",
    "        if v[j] < temp[i]\n",
    "            v[k] = v[j]\n",
    "            j += 1\n",
    "        else\n",
    "            v[k] = temp[i]\n",
    "            i += 1\n",
    "        end\n",
    "        k += 1\n",
    "    end\n",
    "    @inbounds while k < j\n",
    "        v[k] = temp[i]\n",
    "        k += 1\n",
    "        i += 1\n",
    "    end\n",
    "\n",
    "    return v\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.785687 seconds (1.91 k allocations: 305.315 MiB)\n"
     ]
    }
   ],
   "source": [
    "a = rand(10^7)\n",
    "@time psort!(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an enormous amount of worked planned on extending the features covered here. The future is looking bright for Julia.\n",
    "\n",
    "# Thanks for a great quarter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0-rc5",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
