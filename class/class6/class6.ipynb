{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's ICME LA/Opt Seminar will be by one of the co-creators of Julia, Viral Shah \n",
    "\n",
    "* Title: On Machine Learning and Programming Languages\n",
    "* Time: 4:30pm Thursday Feb 15, 2018\n",
    "* Place: Y2E2, Room 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "Today we'll talk a bit about performance, in particular how to write Julia code with performance in mind, and how to measure performance.  You can find a lot of useful material in Julia's documentation: [FAQ](https://docs.julialang.org/en/stable/manual/faq/), [performance tips](https://docs.julialang.org/en/stable/manual/performance-tips/), [profiling](http://docs.julialang.org/en/https://docs.julialang.org/en/stable/manual/profile//manual/profile/).\n",
    "\n",
    "We won't talk too much about performance relative to other languages (see Julia's figure [here](http://julialang.org/benchmarks/), and look around the internet for criticisms), and mostly concern ourselves with writing fast code within Julia.  However, many of these topics apply directly or indirectly to many languages used for scientific computing, so keep an eye out for questions or connections to languages you are familiar with.\n",
    "\n",
    "A few things that we'll pay attention to today are speed and memory allocation.  A few general heuristics are:\n",
    "* Preallocation is good (don't grow arrays dynamically if avoidable)\n",
    "* Type annotations are good (tell the compiler which types you want to instantiate)\n",
    "* Avoid changing the type of variables\n",
    "* Write multiple function methods instead of multiple code paths in a function\n",
    "* Use for-loops over vectorized notation (we saw this last time with array broadcasts)\n",
    "\n",
    "It is worth mentioning that if you don't want to worry about this sort of thing, that's OK.  One of the nice things about Julia is that you can use it at a high level without getting bogged down in this sort of analysis.  However, if you use certain functions a lot, plan on having others use your functions a lot, or want your simulations to finish faster, it may be worth taking a second pass at your code to find optimizations.  Also, if you practice a bit, you will also be able to write code faster the first time around.\n",
    "\n",
    "## Notes on Timing\n",
    "\n",
    "* Remember the first time you run a function it is \"just in time compiled\", meaning if you run it a second time you'll have a better idea of how fast the function actually is.\n",
    "\n",
    "* The actual amount of time it takes to run a function depends on how fast you are able to schedule a function call on your machine.  Remember, you're not just running Julia on your computer, but also an operating system, and perhaps a variety of other applications, all sharing your processor's time and attention.  Usually the best way to make this effect negligable is to amortize it over many function runs by calling a function many times in succession.\n",
    "\n",
    "* Your processor will have a big impact on performance.  Clock speed is the most obvious relevant variable, but architecture and compiler support for your arcitecture also matter (in this case LLVM support).\n",
    "\n",
    "## Example: The Inner Product \n",
    "\n",
    "You've probably already used the `@time` macro, which is a very easy way to get an idea of speed and memory allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function bad_innerprod(x, y)\n",
    "    @assert length(x) == length(y)\n",
    "    ans = 0\n",
    "    for i = 1:length(x)\n",
    "        ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "x = randn(n)\n",
    "y = randn(n)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time bad_innerprod(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `@time` macro will tell you both time information and memory allocation information.  The function above is allocating several kilobytes of memory just to do some simple artihmetic.  This may not seem like a big deal, but imagine if this function was a very small component of a much larger program and was called thousands of times.  Here's a better example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function better_innerprod0{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    ans = zero(T)\n",
    "    for i = 1:length(x)\n",
    "        ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time better_innerprod0(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially just by providing type information, we were able to keep the compiler from allocating unneccssary amounts of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function better_innerprod1{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    ans = zero(T)\n",
    "    for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time better_innerprod1(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@inbounds` macro is saying that the program doesn't need to check that we may try to access a memory location that isn't part of the array.  The complier may be able to infer this in this particular example, but if you have more complicated loops, the macro may give you a noticeable speedup.\n",
    "\n",
    "Now, we separate the inner loop from bounds checking.  If you have more complex logic, breaking your functions into smaller components can speed up evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fast_innerprod{T}(x::Array{T}, y::Array{T})\n",
    "    ans::T = 0\n",
    "    for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    ans\n",
    "end\n",
    "\n",
    "function better_innerprod2{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    fast_innerprod(x, y)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time fast_innerprod(x, y)\n",
    "@time better_innerprod2(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@simd` macro (\"Single Instruction, Multiple Data\") can be used in loops that can be vectorized.  This means no `break`s or `continue`s, and that the loop should not depend on previous loop evaluations.  See more [here](https://en.wikipedia.org/wiki/SIMD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fast_innerprod2{T}(x::Array{T}, y::Array{T})\n",
    "    ans::T = 0\n",
    "    @simd for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    ans\n",
    "end\n",
    "\n",
    "function better_innerprod3{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    fast_innerprod2(x, y)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time fast_innerprod2(x, y)\n",
    "@time better_innerprod3(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the equivalent of the `-ffast-math` compiler optimization flag with `@fastmath`.  Note that this may change the accuracy of your results, or give you an answer that is entirely wrong if you aren't careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fast_innerprod3{T}(x::Array{T}, y::Array{T})\n",
    "    ans::T = 0\n",
    "    @fastmath @simd for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    ans\n",
    "end\n",
    "\n",
    "function better_innerprod4{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    fast_innerprod3(x, y)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time fast_innerprod3(x, y)\n",
    "@time better_innerprod4(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you would compute an inner product with Julia's built in dot: (note that this is calling BLAS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time x' * y \n",
    "@time dot(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the efficiency of each implementation.  The `@elapsed` macro keeps track of how much time is spent in a for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gflop_innerprod( n, reps )\n",
    "    x = randn(n)\n",
    "    y = randn(n)\n",
    "    s = zero(Float64)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=bad_innerprod(x,y)\n",
    "    end\n",
    "    println(\"GFlop (bad_innerprod)      = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod0(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod0)  = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod1(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod1)  = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod2(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod2)  = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod3(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod3)  = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod4(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod4)  = \",2.0*n*reps/time*1E-9)  \n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=dot(x,y)\n",
    "    end\n",
    "    println(\"GFlop (dot)                = \",2.0*n*reps/time*1E-9)\n",
    "        time = @elapsed for j in 1:reps\n",
    "        s+=x' * y\n",
    "    end\n",
    "    println(\"GFlop (array inner prod)   = \",2.0*n*reps/time*1E-9)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gflop_innerprod(2000, 1000)\n",
    "println(\"\")\n",
    "gflop_innerprod(2000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "* Create a function to compute the [smooth maximum](https://en.wikipedia.org/wiki/Smooth_maximum) of an array - `sum(x.*exp.(x))/sum(exp.(x))`  Make one version that is relatively inneficient and one version that is as fast as you can make it.\n",
    "* (if you have time) If you make the binary operation (`+`) in the defintion of dot product a parameter of your function can you still get reasonable performance?   Try the [bitwise `xor`](https://docs.julialang.org/en/stable/manual/mathematical-operations/#Bitwise-Operators-1) on an array of ints.  Compare this to the [reduce function](https://docs.julialang.org/en/stable/stdlib/collections/#Base.reduce-Tuple{Any,Any,Any})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Arrays \n",
    "## Fusing Dot Operations\n",
    "\n",
    "Writing explicit for-loops is one way to make code fast.  Last week we saw how to broadcast a funciton using dot operations.  We can [fuse multiple vectorized functions](https://docs.julialang.org/en/stable/manual/performance-tips/#More-dots:-Fuse-vectorized-operations-1) using the macro `@.`  This is partly a convenience that lets you avoid writing a `.` after each function, but also insurance to make sure you get the most out of vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 3x.^2 + 4x + 7x.^3\n",
    "fdot(x) = @. 3x^2 + 4x + 7x^3\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10^6\n",
    "x = rand(n)\n",
    "@time f(x);\n",
    "@time fdot(x);\n",
    "@time f.(x);\n",
    "@time map(f,x)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views\n",
    "\n",
    "Views of arrays access a sub-array in memory (without making a copy).  If you want to perform an operation on a subarray, views can remove the cost of copying an array.  For more information see [here](https://docs.julialang.org/en/stable/manual/performance-tips/#Consider-using-views-for-slices-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcopy(x) = sum(x[2:end-1])\n",
    "@views fview(x) = sum(x[2:end-1])\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10^6\n",
    "x = rand(n)\n",
    "\n",
    "@time fcopy(x)\n",
    "@time fview(x)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(look at total allocation size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling Code\n",
    "\n",
    "Julia has a [built-in profiler](https://docs.julialang.org/en/stable/manual/profile/#Profiling-1) which will allow you to see where your functions are spending most of their time.  If you have a script or function that is taking a long time to complete, this can help you identify where you should focus your optimization efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function test_fn()\n",
    "    A = randn(1000, 1000)\n",
    "    b  = randn(1000)\n",
    "    c = A * b\n",
    "    maximum(c)\n",
    "end\n",
    "\n",
    "test_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile test_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profile.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@profile` macro will run the function several times, randomly interrupting the call and looking at the stack.  The first number in each line is the number of times the function was found on the call stack.  The rest of the line gives you information on the function and where to find it in the code base.  The output is indented based on where in the stack the function was found.\n",
    "\n",
    "If you want to increase the number of samples, you can put your function in a for-loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile for i = 1:100 test_fn() end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profile.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to go beyond the built in profiler, there's a package that will graphically interperet the results called [ProfileView](https://github.com/timholy/ProfileView.jl).  You can [track memory allocation](https://docs.julialang.org/en/stable/manual/profile/#Memory-allocation-analysis-1) for each line of code by starting up Julia with `--track-allocation=<setting>`.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "* Use views and broadcasting to implement the [xor swap algorithm](https://en.wikipedia.org/wiki/XOR_swap_algorithm) on `X[inds]`, `Y[inds]`, where `X` and `Y` are arrays of the same type and size, and `inds` is a common subarray block. \n",
    "* Use Julia's profiler on the package of your choice.  What's taking the most time?  If you want a starting point, try PyCall.\n",
    "* (if you have time) Try profiling your fast matrix types from HW 2.  Where would you focus your efforts if you wanted increased speed?\n",
    "\n",
    "## More speed\n",
    "\n",
    "### [Type Definitions](https://docs.julialang.org/en/stable/manual/performance-tips/#Type-declarations-1)\n",
    "\n",
    "When you declare types, you should (whenever possible) make fields a concrete type, not even a specific abstract type.  If you want to allow for multiple types in the field, parameterize your type.  If there is any ambiguity in what the actual instantiated type will be, the compiler will not be able to allocate space correctly, and will generally miss out on optimizations.\n",
    "\n",
    "For more about type stability, check out the [`@code_warntype` macro](https://docs.julialang.org/en/stable/manual/performance-tips/#man-code-warntype-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct AmbiguousType\n",
    "    x\n",
    "end\n",
    "\n",
    "mutable struct StillAmbiguousType\n",
    "    x::Real\n",
    "end\n",
    "\n",
    "mutable struct NonAmbiguousType\n",
    "    x::Float64 \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "for T in (AmbiguousType, StillAmbiguousType, NonAmbiguousType)\n",
    "    @time a = Array{T}(n)\n",
    "    t1 = @elapsed for i=1:n\n",
    "        a[i] = T(randn())\n",
    "    end\n",
    "    println(\"$t1 seconds to fill array\")\n",
    "    s = T(0)\n",
    "    t2 = @elapsed for i=1:n\n",
    "        s.x += a[i].x\n",
    "    end\n",
    "    println(\"$t2 seconds to sum array\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Declaration\n",
    "\n",
    "When you use arrays, you should pre-allocate if possible.  Specific type information is also valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "@time a1 = Real[] # Abstract type\n",
    "@time a2 = Float64[] # specific type\n",
    "@time a3 = Array{Real}(n) # pre-allocated abstract type\n",
    "@time a4 = Array{Float64}(n) # pre-allocated specific type\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = @elapsed for i=1:n\n",
    "    push!(a1,randn())\n",
    "end\n",
    "println(\"$t1 seconds to fill a1\")\n",
    "t2 = @elapsed for i=1:n\n",
    "    push!(a2,randn())\n",
    "end\n",
    "println(\"$t2 seconds to fill a2\")\n",
    "t3 = @elapsed for i=1:n\n",
    "    @inbounds a3[i] = randn()\n",
    "end\n",
    "println(\"$t3 seconds to fill a3\")\n",
    "t4 = @elapsed for i=1:n\n",
    "    @inbounds a4[i] = randn()\n",
    "end\n",
    "println(\"$t4 seconds to fill a4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subnormal Numbers\n",
    "\n",
    "You can treat sub-normal numbers as zero.  If a number is less than what can be represented using floating point, your computer may still represent it, and incur performance penalites (although this is required for IEEE standards, so be careful).  See [Denormal Numbers](https://en.wikipedia.org/wiki/Denormal_number) on Wikipedia for more info. The following example is from [Julia's documentation](https://docs.julialang.org/en/stable/manual/performance-tips/#treat-subnormal-numbers-as-zeros), and models the heat equation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function timestep{T}( b::Vector{T}, a::Vector{T}, Δt::T )\n",
    "    @assert length(a)==length(b)\n",
    "    n = length(b)\n",
    "    b[1] = 1                            # Boundary condition\n",
    "    for i=2:n-1\n",
    "        b[i] = a[i] + (a[i-1] - T(2)*a[i] + a[i+1]) * Δt\n",
    "    end\n",
    "    b[n] = 0                            # Boundary condition\n",
    "end\n",
    "\n",
    "function heatflow{T}( a::Vector{T}, nstep::Integer )\n",
    "    b = similar(a)\n",
    "    for t=1:div(nstep,2)                # Assume nstep is even\n",
    "        timestep(b,a,T(0.1))\n",
    "        timestep(a,b,T(0.1))\n",
    "    end\n",
    "end\n",
    "\n",
    "heatflow(zeros(Float32,10),2)           # Force compilation\n",
    "for trial=1:6\n",
    "    a = zeros(Float32,1000)\n",
    "    set_zero_subnormals(iseven(trial))  # Odd trials use strict IEEE arithmetic\n",
    "    @time heatflow(a,1000)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Access arrays in column-major order](https://docs.julialang.org/en/stable/manual/performance-tips/#Access-arrays-in-memory-order,-along-columns-1)\n",
    "If you need to loop over an array, keep in mind that it is stored in column-major format, so looping over indices in reverse order will allow you to use blocks of memory more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access in Column-major order\n",
    "function sum_array1{T}(A::Array{T,3})\n",
    "    s::T = 0\n",
    "    @simd for k=1:size(A,3)\n",
    "        @simd for j=1:size(A,2)\n",
    "            @simd for i=1:size(A,1)\n",
    "                @inbounds s += A[i,j,k]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "# access in Row-major order\n",
    "function sum_array2{T}(A::Array{T,3})\n",
    "    s::T = 0\n",
    "    @simd for i=1:size(A,1)\n",
    "        @simd for j=1:size(A,2)\n",
    "            @simd for k=1:size(A,3)\n",
    "                @inbounds s += A[i,j,k]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return s\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300\n",
    "A = rand(Int64,n,n,n)\n",
    "@time sum_array1(A)\n",
    "@time sum_array2(A)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1 2; 3 4]\n",
    "@show A\n",
    "@show A[:]\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Minor Tweaks](https://docs.julialang.org/en/stable/manual/performance-tips/#Tweaks-1)\n",
    "\n",
    "Julia's performance documentation suggests the following optimizations for making very fast inner loops:\n",
    "\n",
    "* Avoid unnecessary arrays. For example, instead of sum([x,y,z]) use x+y+z.\n",
    "* Use `abs2(z)` instead of `abs(z)^2` for complex z. In general, try to rewrite code to use `abs2()` instead of `abs()` for complex arguments. (This would be useful for writing fast Julia Set codes!)\n",
    "* Use `div(x,y)` for truncating division of integers instead of `trunc(x/y)`, `fld(x,y)` instead of `floor(x/y)`, and `cld(x,y)` instead of `ceil(x/y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "@time z = Array{Complex{Float64}}(n)\n",
    "t = @elapsed for i = 1:n\n",
    "    @inbounds z[i] = Complex{Float64}(randn(),randn())\n",
    "end\n",
    "println(\"$t sec to initialize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnd = 0.5\n",
    "s = zero(Int64)\n",
    "t = @elapsed for i = 1:n\n",
    "    @inbounds s += (abs(z[i]) > bnd ? one(Int64) : zero(Int64))\n",
    "end\n",
    "println(\"$t sec using abs()\")\n",
    "bnd2 = bnd*bnd\n",
    "s = zero(Int64)\n",
    "t = @elapsed for i = 1:n\n",
    "    @inbounds s += (abs2(z[i]) > bnd2 ? one(Int64) : zero(Int64))\n",
    "end\n",
    "println(\"$t sec using abs2()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "* Write a standard matrix-vector multiplication function on two arrays.  Can you get close to the default implementation's performance (BLAS gemv)?\n",
    "* How would you modify your routine to do a matrix transpose-vector multiplication routine? \n",
    "* Why do you think [BLAS's gemv](http://www.netlib.org/lapack/explore-html/dc/da8/dgemv_8f.html) takes the arguments that it does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Performance Analysis\n",
    "\n",
    "* [Lint](https://github.com/tonyhffong/Lint.jl) - analyze code for potential improvements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
