{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "Today we'll talk a bit about performance, in particular how to write Julia code with performance in mind, and how to measure performance.  You can find a lot of useful material in Julia's documentation: , [performance tips](https://docs.julialang.org/en/v1/manual/performance-tips/index.html), [profiling](https://docs.julialang.org/en/v1/manual/profile/).\n",
    "\n",
    "We won't talk too much about performance relative to other languages (see Julia's figure [here](http://julialang.org/benchmarks/), and look around the internet for criticisms), and mostly concern ourselves with writing fast code within Julia.  However, many of these topics apply directly or indirectly to many languages used for scientific computing, so keep an eye out for questions or connections to languages you are familiar with.\n",
    "\n",
    "A few things that we'll pay attention to today are speed and memory allocation.  A few general heuristics are:\n",
    "* Preallocation is good (don't grow arrays dynamically if avoidable)\n",
    "* Type annotations are good (tell the compiler which types you want to instantiate)\n",
    "* Avoid changing the type of variables\n",
    "* Write multiple function methods instead of multiple code paths in a function\n",
    "* It is (usually) faster to write for loops  than to use vectorization if you're doing anything complicated\n",
    "* When modifying a global variable with a complicated operation, update a local variable instead and update the global variable at the end\n",
    "\n",
    "It is worth mentioning that if you don't want to worry about this sort of thing, that's OK.  One of the nice things about Julia is that you can use it at a high level without getting bogged down in this sort of analysis.  However, if you use certain functions a lot, plan on having others use your functions a lot, or want your simulations to finish faster, it may be worth taking a second pass at your code to find optimizations.  Also, if you practice a bit, you will also be able to write code faster the first time around.\n",
    "\n",
    "## Notes on Timing\n",
    "\n",
    "* Remember the first time you run a function it is \"just in time compiled\", meaning if you run it a second time you'll have a better idea of how fast the function actually is.\n",
    "\n",
    "* The actual amount of time it takes to run a function depends on how fast you are able to schedule a function call on your machine.  Remember, you're not just running Julia on your computer, but also an operating system, and perhaps a variety of other applications, all sharing your processor's time and attention.  Usually the best way to make this effect negligable is to amortize it over many function runs by calling a function many times in succession.\n",
    "\n",
    "* Since the overhead induced by your operating system is essentially random, sometimes a few tests with @time will indicate that a change to the code makes it faster, even if it is actually a performance regression. To get around this issue we will be using `BenchmarkTools.jl` to test our code for this lecture. You can install it with `using Pkg ; Pkg.add(\"BenchmarkTools\") ; using BenchmarkTools`.\n",
    "\n",
    "* Your processor will have a big impact on performance.  Clock speed is the most obvious relevant variable, but architecture and compiler support for your arcitecture also matter (in this case LLVM support).\n",
    "\n",
    "## Example: The Inner Product\n",
    "\n",
    "Here is a fairly straightforward way to implement the inner product of two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function slow_innerprod(x, y)\n",
    "    @assert length(x) == length(y)\n",
    "    ans = 0\n",
    "    for i = 1:length(x)\n",
    "        ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "n = 10000\n",
    "x = randn(n)\n",
    "y = randn(n)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time slow_innerprod(x,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark slow_innerprod(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, both the `@time` and `@benchmark` macros will tell you both time information and memory allocation information. Although we did not provide Julia any typing information in this example, Julia was able to infer that we were doing operations on floating-point numbers at runtime and avoid allocating a large amount of memory. However on more complicated functions Julia will not be able to do this and consequently our performance will suffer as a consequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function better_innerprod(x::Array{T}, y::Array{T})::T where T\n",
    "    @assert length(x) == length(y)\n",
    "    ans = zero(T)\n",
    "    for i = 1:length(x)\n",
    "        ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark better_innerprod(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially just by providing type information, we were able to keep the compiler from allocating unneccssary amounts of memory, and obtain a slight performance boost. This is not that impressive in the case of this simple dot product function, but for more complicated programs Julia's compiler will not be able to infer types as effectively-- the difference will be more significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a more complicated function where type information helps significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function merge_slow(x,y)\n",
    "    output = Array{Any}(undef,length(x)+length(y))\n",
    "    x_ind = 1; y_ind = 1\n",
    "    output_ind = 1\n",
    "    while x_ind <= length(x) || y_ind <= length(y)\n",
    "        if y_ind > length(y)\n",
    "            output[output_ind] = x[x_ind]\n",
    "            x_ind += 1\n",
    "        elseif x_ind > length(x) || x[x_ind] > y[y_ind]\n",
    "            output[output_ind] = y[y_ind]\n",
    "            y_ind += 1\n",
    "        else\n",
    "            output[output_ind] = x[x_ind]\n",
    "            x_ind += 1\n",
    "        end\n",
    "        output_ind += 1\n",
    "    end\n",
    "    return output\n",
    "end\n",
    "     \n",
    "function mergesort_slow(x)\n",
    "    if length(x) == 1 \n",
    "        return x\n",
    "    end\n",
    "    split = div(length(x),2)\n",
    "    y = mergesort_slow(x[1:split])\n",
    "    z = mergesort_slow(x[split+1:end])\n",
    "    return merge_slow(y,z)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = rand(10000)\n",
    "@benchmark mergesort_slow(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function merge_fast(x::Array{T,1},y::Array{T,1})::Array{T,1} where T\n",
    "    output = Array{T}(undef,length(x)+length(y))\n",
    "    x_ind = 1; y_ind = 1\n",
    "    output_ind = 1\n",
    "    while x_ind <= length(x) || y_ind <= length(y)\n",
    "        if y_ind > length(y)\n",
    "            output[output_ind] = x[x_ind]\n",
    "            x_ind += 1\n",
    "        elseif x_ind > length(x) || x[x_ind] > y[y_ind]\n",
    "            output[output_ind] = y[y_ind]\n",
    "            y_ind += 1\n",
    "        else\n",
    "            output[output_ind] = x[x_ind]\n",
    "            x_ind += 1\n",
    "        end\n",
    "        output_ind += 1\n",
    "    end\n",
    "    return output\n",
    "end\n",
    "     \n",
    "function mergesort_fast(x::Array{T,1})::Array{T,1} where T\n",
    "    if length(x) == 1 \n",
    "        return x\n",
    "    end\n",
    "    split = div(length(x),2)\n",
    "    y = mergesort_fast(x[1:split])\n",
    "    z = mergesort_fast(x[split+1:end])\n",
    "    return merge_fast(y,z)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = rand(10000)\n",
    "@benchmark mergesort_fast(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferring the type `T` rather than using `Any` vectors gives a significant speedup.\n",
    "\n",
    "Back to inner products..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function betterer_innerprod(x::Array{T}, y::Array{T})::T where T\n",
    "    @assert length(x) == length(y)\n",
    "    ans = zero(T)\n",
    "    for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark betterer_innerprod(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@inbounds` macro is saying that the program doesn't need to check that we may try to access a memory location that isn't part of the array.  The complier may be able to infer this in this particular example, but if you have more complicated loops, the macro may give you a noticeable speedup.\n",
    "\n",
    "Now, we separate the inner loop from bounds checking.  If you have more complex logic, breaking your functions into smaller components can speed up evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fast_innerprod(x::Array{T}, y::Array{T})::T where T\n",
    "    ans::T = 0\n",
    "    for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    "\n",
    "function better_innerprod1(x::Array{T}, y::Array{T})::T where T\n",
    "    @assert length(x) == length(y)\n",
    "    return fast_innerprod(x, y)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark better_innerprod1(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function faster_innerprod(x::Array{T}, y::Array{T})::T where T\n",
    "    ans::T = 0\n",
    "    @simd for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    "\n",
    "function better_innerprod2(x::Array{T}, y::Array{T})::T where T\n",
    "    @assert length(x) == length(y)\n",
    "    return faster_innerprod(x, y)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark better_innerprod2(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@simd` macro (\"Single Instruction, Multiple Data\") can be used in loops that can be vectorized.  This means no `break`s or `continue`s, and that the loop should not depend on previous loop evaluations.  See more [here](https://en.wikipedia.org/wiki/SIMD). As you can see the improvement from vectorizing our code dwarfs anything else we could do for this simple example. Vectorization is powerful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the equivalent of the `-ffast-math` compiler optimization flag with `@fastmath`.  Note that this may change the accuracy of your results, or give you an answer that is entirely wrong if you aren't careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fastest_innerprod(x::Array{T}, y::Array{T})::T where T\n",
    "    ans::T = 0\n",
    "    @fastmath @simd for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    ans\n",
    "end\n",
    "\n",
    "function better_innerprod3(x::Array{T}, y::Array{T})::T where T\n",
    "    @assert length(x) == length(y)\n",
    "    fastest_innerprod(x, y)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark better_innerprod3(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you would compute an inner product with Julia's built in dot: (note that this is calling BLAS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark x' * y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark dot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our implementation performs roughly as well as Julia's native functions for inner products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "* Modify the slowest and fastest versions of our dot product function to write a function which sums the entries of a vector. Compare their performances with `@benchmark`.\n",
    "* Create a function to compute the [smooth maximum](https://en.wikipedia.org/wiki/Smooth_maximum) of an array - `sum(x.*exp.(x))/sum(exp.(x))`  Make one version that is relatively inefficient and one version that is as fast as you can make it.\n",
    "* (if you have time) If you make the binary operation (`+`) in the defintion of dot product a parameter of your function can you still get reasonable performance?   Try the [bitwise `xor`](https://docs.julialang.org/en/stable/manual/mathematical-operations/#Bitwise-Operators-1) on an array of ints.  Compare this to the [reduce function](https://docs.julialang.org/en/stable/stdlib/collections/#Base.reduce-Tuple{Any,Any,Any})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Arrays \n",
    "## Fusing Dot Operations\n",
    "\n",
    "Writing explicit for-loops is one way to make code fast.  Last week we saw how to broadcast a funciton using dot operations.  We can [fuse multiple vectorized functions](https://docs.julialang.org/en/stable/manual/performance-tips/#More-dots:-Fuse-vectorized-operations-1) using the macro `@.`  This is partly a convenience that lets you avoid writing a `.` after each function, but also insurance to make sure you get the most out of vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 3x.^2 + 4x + 7x.^3\n",
    "fdot(x) = @. 3x^2 + 4x + 7x^3\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10^6\n",
    "x = rand(n)\n",
    "@time f(x);\n",
    "@time fdot(x);\n",
    "@time f.(x);\n",
    "@time map(f,x)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views\n",
    "\n",
    "Views of arrays access a sub-array in memory (without making a copy).  If you want to perform an operation on a subarray, views can remove the cost of copying an array.  For more information see [here](https://docs.julialang.org/en/stable/manual/performance-tips/#Consider-using-views-for-slices-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcopy(x) = sum(x[2:end-1])\n",
    "@views fview(x) = sum(x[2:end-1])\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10^6\n",
    "x = rand(n)\n",
    "\n",
    "@time fcopy(x)\n",
    "@time fview(x)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(look at total allocation size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoping\n",
    "\n",
    "Another thing to be aware of is how `global` scope in Julia affects your code's performance. While read-only access to `global` variables from a function is allowed, type inference on these does not work: Julia in `local` scope treats all `globals` as `Any` type. You can get around this issue by using the keyword `const`: this allows Julia to infer the type properly. However, the scoping rules in notebooks are different from those on the REPL and in scripts, so you'll have to take my word for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling Code\n",
    "\n",
    "Julia has a [built-in profiler](https://docs.julialang.org/en/stable/manual/profile/#Profiling-1) which will allow you to see where your functions are spending most of their time.  If you have a script or function that is taking a long time to complete, this can help you identify where you should focus your optimization efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Profile\n",
    "function test_fn()\n",
    "    A = randn(1000, 1000)\n",
    "    b  = randn(1000)\n",
    "    c = A * b\n",
    "    maximum(c)\n",
    "end\n",
    "\n",
    "test_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile test_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profile.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@profile` macro will run the function several times, randomly interrupting the call and looking at the stack.  The first number in each line is the number of times the function was found on the call stack.  The rest of the line gives you information on the function and where to find it in the code base.  The output is indented based on where in the stack the function was found.\n",
    "\n",
    "If you want to increase the number of samples, you can put your function in a for-loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profile.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile for i = 1:100 test_fn() end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profile.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to go beyond the built in profiler, there's a package that will graphically interperet the results called [ProfileView](https://github.com/timholy/ProfileView.jl).  You can [track memory allocation](https://docs.julialang.org/en/stable/manual/profile/#Memory-allocation-analysis-1) for each line of code by starting up Julia with `--track-allocation=<setting>`.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "* Use views and broadcasting to implement the [xor swap algorithm](https://en.wikipedia.org/wiki/XOR_swap_algorithm) on `X[inds]`, `Y[inds]`, where `X` and `Y` are arrays of the same type and size, and `inds` is a common subarray block. \n",
    "* Use Julia's profiler on the package of your choice.  What's taking the most time?  If you want a starting point, try PyCall.\n",
    "* (if you have time) Try profiling your fast matrix types from HW 2.  Where would you focus your efforts if you wanted increased speed?\n",
    "\n",
    "## More speed\n",
    "\n",
    "### [Type Definitions](https://docs.julialang.org/en/stable/manual/performance-tips/#Type-declarations-1)\n",
    "\n",
    "When you declare types, you should (whenever possible) make fields a concrete type, not even a specific abstract type.  If you want to allow for multiple types in the field, parameterize your type.  If there is any ambiguity in what the actual instantiated type will be, the compiler will not be able to allocate space correctly, and will generally miss out on optimizations.\n",
    "\n",
    "For more about type stability, check out the [`@code_warntype` macro](https://docs.julialang.org/en/stable/manual/performance-tips/#man-code-warntype-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function swapsub!(X::Array{T}, Y::Array{T}, inds) where T\n",
    "    @views @. X[inds] = xor(X[inds], Y[inds])\n",
    "    @views @. Y[inds] = xor(X[inds], Y[inds])\n",
    "    @views @. X[inds] = xor(X[inds], Y[inds])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1; 2; 3]\n",
    "y = [4; 5; 6]\n",
    "@time swapsub!(x, y, 1:2)\n",
    "@show x\n",
    "@show y\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct AmbiguousType\n",
    "    x\n",
    "end\n",
    "\n",
    "mutable struct StillAmbiguousType\n",
    "    x::Real\n",
    "end\n",
    "\n",
    "mutable struct NonAmbiguousType\n",
    "    x::Float64 \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "for T in (AmbiguousType, StillAmbiguousType, NonAmbiguousType)\n",
    "    @time a = Array{T}(undef,n)\n",
    "    t1 = @elapsed for i=1:n\n",
    "        a[i] = T(randn())\n",
    "    end\n",
    "    println(\"$t1 seconds to fill array\")\n",
    "    s = T(0)\n",
    "    t2 = @elapsed for i=1:n\n",
    "        s.x += a[i].x\n",
    "    end\n",
    "    println(\"$t2 seconds to sum array\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Declaration\n",
    "\n",
    "When you use arrays, you should pre-allocate if possible.  Specific type information is also valuable. However, an abstract vector (eg. `Real[]`) is actually no better than a vector of `Any` `Any[]` for the sake of performance-- it is simply an organizational tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "@time a1 = Real[] # Abstract type\n",
    "@time a2 = Float64[] # specific type\n",
    "@time a3 = Array{Real}(undef,n) # pre-allocated abstract type\n",
    "@time a4 = Array{Float64}(undef,n) # pre-allocated specific type\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fill_push(a)\n",
    "    for i = 1:n\n",
    "        push!(a,rand())\n",
    "    end\n",
    "    return a\n",
    "end\n",
    "\n",
    "\n",
    "function fill_inbounds(a)\n",
    "    for i = 1:n\n",
    "        @inbounds a[i] = rand()\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time fill_inbounds(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time fill_inbounds(a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time fill_push(a1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subnormal Numbers\n",
    "\n",
    "You can treat sub-normal numbers as zero.  If a number is less than what can be represented using floating point, your computer may still represent it, and incur performance penalites (although this is required for IEEE standards, so be careful).  See [Denormal Numbers](https://en.wikipedia.org/wiki/Denormal_number) on Wikipedia for more info. The following example is from [Julia's documentation](https://docs.julialang.org/en/stable/manual/performance-tips/#treat-subnormal-numbers-as-zeros), and models the heat equation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function timestep(b::Vector{T}, a::Vector{T}, Δt::T ) where T\n",
    "    @assert length(a)==length(b)\n",
    "    n = length(b)\n",
    "    b[1] = 1                            # Boundary condition\n",
    "    for i=2:n-1\n",
    "        b[i] = a[i] + (a[i-1] - T(2)*a[i] + a[i+1]) * Δt\n",
    "    end\n",
    "    b[n] = 0                            # Boundary condition\n",
    "end\n",
    "\n",
    "function heatflow( a::Vector{T}, nstep::Integer ) where T\n",
    "    b = similar(a)\n",
    "    for t=1:div(nstep,2)                # Assume nstep is even\n",
    "        timestep(b,a,T(0.1))\n",
    "        timestep(a,b,T(0.1))\n",
    "    end\n",
    "end\n",
    "\n",
    "heatflow(zeros(Float32,10),2)           # Force compilation\n",
    "for trial=1:6\n",
    "    a = zeros(Float32,1000)\n",
    "    set_zero_subnormals(iseven(trial))  # Odd trials use strict IEEE arithmetic\n",
    "    @time heatflow(a,1000)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Access arrays in column-major order](https://docs.julialang.org/en/stable/manual/performance-tips/#Access-arrays-in-memory-order,-along-columns-1)\n",
    "If you need to loop over an array, keep in mind that it is stored in column-major format, so looping over indices in reverse order will allow you to use blocks of memory more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access in Column-major order\n",
    "function sum_array1(A::Array{T,3}) where T\n",
    "    s::T = 0\n",
    "    @simd for k=1:size(A,3)\n",
    "        @simd for j=1:size(A,2)\n",
    "            @simd for i=1:size(A,1)\n",
    "                @inbounds s += A[i,j,k]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "# access in Row-major order\n",
    "function sum_array2(A::Array{T,3}) where T\n",
    "    s::T = 0\n",
    "    @simd for i=1:size(A,1)\n",
    "        @simd for j=1:size(A,2)\n",
    "            @simd for k=1:size(A,3)\n",
    "                @inbounds s += A[i,j,k]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return s\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: sum_array1 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: sum_array1 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at util.jl:156",
      " [2] top-level scope at In[1]:3"
     ]
    }
   ],
   "source": [
    "n = 300\n",
    "A = rand(Int64,n,n,n)\n",
    "@time sum_array1(A)\n",
    "@time sum_array2(A)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [1 2; 3 4]\n",
      "A[:] = [1, 3, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "A = [1 2; 3 4]\n",
    "@show A\n",
    "@show A[:]\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Minor Tweaks](https://docs.julialang.org/en/stable/manual/performance-tips/#Tweaks-1)\n",
    "\n",
    "Julia's performance documentation suggests the following optimizations for making very fast inner loops:\n",
    "\n",
    "* Avoid unnecessary arrays. For example, instead of sum([x,y,z]) use x+y+z.\n",
    "* Use `abs2(z)` instead of `abs(z)^2` for complex z. In general, try to rewrite code to use `abs2()` instead of `abs()` for complex arguments. (This would be useful for writing fast Julia Set codes!)\n",
    "* Use `div(x,y)` for truncating division of integers instead of `trunc(x/y)`, `fld(x,y)` instead of `floor(x/y)`, and `cld(x,y)` instead of `ceil(x/y)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "* Write a standard matrix-vector multiplication function on two arrays.  Can you get close to the default implementation's performance (BLAS gemv)?\n",
    "* How would you modify your routine to do a matrix transpose-vector multiplication routine? \n",
    "* Why do you think [BLAS's gemv](http://www.netlib.org/lapack/explore-html/dc/da8/dgemv_8f.html) takes the arguments that it does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Performance Analysis\n",
    "\n",
    "* [Lint](https://github.com/tonyhffong/Lint.jl) - analyze code for potential improvements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
